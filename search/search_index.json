{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TrinityLake","text":"<p>TrinityLake is an Open Lakehouse Format for Big Data Analytics, ML &amp; AI.  It defines a storage layout on top of objects like Apache Iceberg tables,  Substrait views, etc. to form a complete storage-only lakehouse.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#storage-only","title":"Storage Only","text":"<p>TrinityLake mainly leverages one storage primitive - mutual exclusion of file creation. This means you can use TrinityLake to build a storage-only lakehouse on  almost any storage solution including Linux file system, open source storage solutions like Apache Hadoop Distributed File System (HDFS) or Apache Ozone, and cloud storage providers like Amazon S3, Google Cloud Storage, Azure Data Lake Storage.</p>"},{"location":"#multi-object-multi-statement-transactions","title":"Multi-Object Multi-Statement Transactions","text":"<p>TrinityLake enables multi-object multi-statement transactions across different tables, indexes, views,  materialized views, etc. within a lakehouse. Users could start to leverage standard SQL BEGIN and COMMIT semantics and expect ACID enforcement  at SNAPSHOT or SERIALIZABLE isolation level across the entire lakehouse.</p>"},{"location":"#consistent-time-travel-rollback-and-snapshot-export","title":"Consistent Time Travel, Rollback and Snapshot Export","text":"<p>TrinityLake provides a single timeline for all the transactions that have taken place within a Lakehouse. Users can perform time travel to get a consistent view of all the objects in the lakehouse, rollback the lakehouse to a consistent previous state, or choose to export a snapshot of the entire lakehouse at any given point of time.</p>"},{"location":"#distributed-transaction-for-advanced-write-audit-publish","title":"Distributed Transaction for Advanced Write-Audit-Publish","text":"<p>A TrinityLake transaction can be executed not just by a single process,  but can be distributed around multiple processes. This could be used for highly complicated write-audit-publish workflows,  where a writer process can first perform any number of operations against any number of objects in a transactions, and pass the full transaction to an auditor process to review, modify and eventually commit.</p>"},{"location":"#integrations","title":"Integrations","text":""},{"location":"#open-table-formats","title":"Open Table Formats","text":"<p>TrinityLake can work with popular open table formats such as Apache Iceberg. Users can create and use these tables with both the traditional SQL <code>MANAGED</code> or <code>EXTERNAL</code> experience, as well as through federation when the table resides in other systems that can be connected to for read and write. See Table for more details.</p>"},{"location":"#open-relational-algebra-formats","title":"Open Relational Algebra Formats","text":"<p>TrinityLake can work with open relational algebra formats such as Substrait. users can use these relational algebra formats to define important objects in a lakehouse such as view or materialized view.</p>"},{"location":"#open-catalog-standards","title":"Open Catalog Standards","text":"<p>TrinityLake can be used as an implementation of open catalog standards like the Apache Iceberg REST Catalog (IRC) standard. The project provides catalog servers like an IRC server that users can run as a proxy to access TrinityLake and leverage all open source and  vendor products that support IRC. This provides a scalable yet lightweight IRC implementation  where the IRC server is mainly just a proxy, and the main execution logic is pushed down to the storage layer and handled by this open lakehouse format. See Catalog Integration for more details.</p>"},{"location":"#open-engines","title":"Open Engines","text":"<p>Through open table formats and open catalog standards, you can use TrinityLake with any open engine that supports them, for example with the Spark Iceberg connector. In addition, TrinityLake provides native connectors to various open engines such as Apache Spark. These native connectors will provide the full TrinityLake experience to users.</p>"},{"location":"community/","title":"Community","text":"<p>Note</p> <p>This project is at very early development stage. We are still working on building the community and improving community guidelines.</p>"},{"location":"community/#slack","title":"Slack","text":"<p>we mainly use Slack (click for invite link)  for day-to-day communications and quick conversations. </p>"},{"location":"community/#github-issues","title":"GitHub Issues","text":"<p>We mainly use GitHub Issues to track ongoing issues. If you are interested in getting started with contributing, take a look at the  good first issues</p>"},{"location":"community/#github-discussions","title":"GitHub Discussions","text":"<p>We mainly use  GitHub Discussions  for discussing complicated topics.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>We mainly use GitHub Project to organize roadmap items. Each GitHub project represents a specific area that TrinityLake would like to explore:</p> Area of Development Description Tree Improvements Improve the TrinityLake tree related algorithm and implementation Storage Improvements Improve storage implementations and add more storage integrations View Add view related objects to the format, like logical view, materialized view, etc. ML &amp; AI Collaboration Add features such as affinity table and affinity view to faciliate cross-team ML &amp; AI collaboration in a lakehouse Access Control Add access control related objects to the format like policy and principal, and integrate with policy language like Cedar Policy Index Add index related objects to the format, such as clustering index, data sketches, vector index, etc. Lock Mechanism Explore locking mechanism improvements, with features like global lock, table lock, etc. Lakehouse Procedures Add various procedures for lakehouse-wide operations such as expiring old versions, export a version, etc. Streaming and Upsert Explore how streaming and upsert behaves in TrinityLake's transaction mechanism Catalog Integration Improve integration with catalog standards, such as the Iceberg REST Catalog standard, Hive Metastore standard, etc. Spark Integration Improve integration with Apache Spark engine Java SDK Develop the Java SDK for the format, used to hold items in Java SDK that are not categorized by other areas yet Python &amp; Rust SDK Develop the Python &amp; Rust SDK for the format, used to hold items in Python &amp; Rust SDK that are not categorized by other areas yet"},{"location":"catalog/iceberg-rest/","title":"Iceberg REST Catalog","text":"<p>TrinityLake provides the ability to build an Iceberg REST Catalog (IRC) server  following the IRC open catalog standard.</p>"},{"location":"catalog/iceberg-rest/#operation-behavior","title":"Operation Behavior","text":"<p>The TrinityLake-backed IRC offers the same operation behavior as the Iceberg catalog integration. See Operation Behavior in Iceberg Catalog for more details.</p>"},{"location":"catalog/iceberg-rest/#using-system-namespace","title":"Using System Namespace","text":"<p>The TrinityLake-backed IRC offers the same system namespace support as the Iceberg catalog integration to perform operations like create lakehouse and list distributed transactions. See Using System Namespace in Iceberg Catalog for more details.</p>"},{"location":"catalog/iceberg-rest/#using-distributed-transaction","title":"Using Distributed Transaction","text":"<p>The TrinityLake-backed IRC offers the same distributed transaction support as the Iceberg catalog integration using multi-level namespace. See Using Distributed Transaction in Iceberg Catalog for more details.</p>"},{"location":"catalog/iceberg-rest/#apache-gravitino-irc-server","title":"Apache Gravitino IRC Server","text":"<p>The easiest way to start a TrinityLake-backed IRC server is to use the Apache Gravitino IRC Server. You can run the Gravitino Iceberg REST server integrated with TrinityLake backend in two ways.</p>"},{"location":"catalog/iceberg-rest/#using-the-prebuilt-docker-image-recommended","title":"Using the Prebuilt Docker Image (Recommended)","text":"<p>Pull the docker image from official trinitylake docker account <pre><code>docker pull trinitylake/trinitylake-gravitino-iceberg-rest-server:latest\n</code></pre></p> <p>Run the Gravitino IRC Server container with port mapping <pre><code>docker run -d -p 9001:9001 --name trinitylake-gravitino-iceberg-rest-server trinitylake/trinitylake-gravitino-iceberg-rest-server\n</code></pre></p>"},{"location":"catalog/iceberg-rest/#using-apache-gravitino-installation-manual-setup","title":"Using Apache Gravitino Installation (Manual Setup)","text":"<p>Follow the Apache Gravitino instructions  for downloading and installing the Gravitino software. After the standard installation process, add the <code>trinitylake-spark-runtime-3.5_2.12-0.0.1.jar</code> to your Java classpath or copy the trinitylake spark runtime jar into Gravitino\u2019s <code>lib/</code> directory . </p>"},{"location":"catalog/iceberg-rest/#configuration","title":"Configuration","text":"<p>Update <code>gravitino-iceberg-rest-server.conf</code> with the following configuration:</p> Configuration Item Description Value gravitino.iceberg-rest.catalog-backend The Catalog backend of the Gravitino Iceberg REST catalog service custom gravitino.iceberg-rest.catalog-backend-impl The Catalog backend implementation of the Gravitino Iceberg REST catalog service. io.trinitylake.iceberg.TrinityLakeIcebergCatalog gravitino.iceberg-rest.catalog-backend-name The catalog backend name passed to underlying Iceberg catalog backend. any name you like, e.g. <code>trinitylake</code> gravitino.iceberg-rest.uri Iceberg REST catalog server address For local development, use <code>http://127.0.0.1:9001</code> gravitino.iceberg-rest.warehouse Trinity lakehouse storage root URI Any file path. Ex: <code>/tmp/trinitylake</code> gravitino.iceberg-rest.&lt;key&gt; Any other catalog properties, see TrinityLake Iceberg catalog properties"},{"location":"catalog/iceberg-rest/#running-the-server","title":"Running the server","text":"<p>To start the Gravitino Iceberg REST server <pre><code>./bin/gravitino-iceberg-rest-server.sh start\n</code></pre></p> <p>To stop the Gravitino Iceberg REST server <pre><code>./bin/gravitino-iceberg-rest-server.sh stop\n</code></pre></p> <p>Follow the Apache Gravitino instructions for more detailed instructions on starting the IRC server and exploring the namespaces, tables and distributed transactions in the catalog.</p>"},{"location":"catalog/iceberg-rest/#examples","title":"Examples","text":"<p>List catalog configuration settings <pre><code>curl -X GET \"http://127.0.0.1:9001/iceberg/v1/config\"\n</code></pre></p> <p>Create lakehouse <pre><code>curl -X POST \"http://127.0.0.1:9001/iceberg/v1/namespaces\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"namespace\": [\"sys\"],\n           \"properties\": {}\n         }'\n</code></pre></p> <p>List namespaces <pre><code>curl -X GET \"http://127.0.0.1:9001/iceberg/v1/namespaces\"\n</code></pre></p>"},{"location":"catalog/iceberg/","title":"Iceberg Catalog","text":"<p>TrinityLake provides an implementation of the pluggable <code>Catalog</code> API standard in Apache Iceberg. Currently we provide the Java implementation <code>io.trinitylake.iceberg.TrinityLakeIcebergCatalog</code>. Python and Rust implementations are work in progress. See related Iceberg documentation for how to use it standalone or  with various query engines like Apache Spark,  Apache Flink, and Apache Hive.</p>"},{"location":"catalog/iceberg/#catalog-properties","title":"Catalog Properties","text":"<p>The TrinityLake Iceberg catalog exposes the following catalog properties:</p> Property Name Description Required? Default warehouse The Iceberg catalog warehouse location, should be set to the root URI for the lakehouse storage Yes storage.type Type of storage. If not set, the type is inferred from the root URI scheme. No storage.ops.&lt;key&gt; Any property configuration for a specific type of storage operation. See Storage for more details. No system.ns-name Name of the system namespace No sys dtxn.parent-ns-name Name of the parent namespace name, this parent namespace is within the system namespace and hold all distributed transaction namespaces No dtxns dtxn.ns-prefix The prefix for a namespace to represent a distributed transaction No dtxn_ <p>For example, a user can initialize a TrinityLake Iceberg catalog with:</p> <pre><code>import io.trinitylake.iceberg.TrinityLakeIcebergCatalog;\nimport io.trinitylake.relocated.com.google.common.collect.ImmutableMap;\nimport org.apache.iceberg.catalog.Catalog;\nimport org.apache.iceberg.catalog.CatalogProperties;\nimport org.apache.iceberg.catalog.SupportsNamespaces;\n\nCatalog catalog = new TrinityLakeIcebergCatalog();\ncatalog.initialize(\n        ImmutableMap.of(\n                CatalogProperties.WAREHOUSE_LOCATION, // \"warehouse\"\n                \"s3://my-bucket\"));\n\nSupportsNamespaces nsCatalog = (SupportsNamespace) catalog;\n</code></pre>"},{"location":"catalog/iceberg/#operation-behavior","title":"Operation Behavior","text":"<p>When using TrinityLake through Iceberg catalog, all the operations will first begin a transaction and then perform the operation. If the operation modifies the object, it will commit the transaction to the lakehouse.</p> <p>When using Iceberg transactions to perform multiple operations, TrinityLake will begin a transaction, perform all the operations and then commit the transaction to the lakehouse. For example:</p> <pre><code>import org.apache.iceberg.catalog.TableIdentifier;\nimport org.apache.iceberg.types.Types;\n\nTable table = catalog.loadTable(TableIdentifier.of(\"ns1\", \"t1\"));\nTransaction txn = table.newTransaction();\n\ntxn.updateSchema()\n    .addColumn(\"region\", Types.StringType.get())\n    .commit();\n\ntxn.updateSpec()\n    .addField(\"region\")\n    .commit();\n\ntxn.commitTransaction();\n</code></pre> <p>In this sequence, a TrinityLake transaction will hold the schema and partition spec update,  and commit both changes to lakehouse in a single transaction. </p>"},{"location":"catalog/iceberg/#using-system-namespace","title":"Using System Namespace","text":"<p>The system namespace is a special namespace with name determined by <code>system.ns-name</code> that exists  if a Trinity lakehouse is initialized at the <code>warehouse</code> location. It contains information about the distributed transactions that are available to use in the lakehouse.</p>"},{"location":"catalog/iceberg/#create-a-new-lakehouse","title":"Create a new lakehouse","text":"<p>If there is no Trinity lakehouse initialized at the <code>warehouse</code> location, the system namespace will not exist. The act of creating this system namespace represents creating the lakehouse. At creation time, lakehouse definition fields can be supplied through the namespace properties.</p> <p>For example:</p> <pre><code>import org.apache.iceberg.Namespace;\n\nnsCatalog.createNamespace(\n        Namespace.of(\"sys\"), \n        ImmutableMap.of(\"namespace_name_max_size_bytes\", \"256\"));\n</code></pre>"},{"location":"catalog/iceberg/#using-distributed-transaction","title":"Using Distributed Transaction","text":"<p>You can use the TrinityLake transaction semantics through Iceberg multi-level namespace.</p>"},{"location":"catalog/iceberg/#distributed-transactions-namespace","title":"Distributed transactions namespace","text":"<p>Under the system namespace, there is always a namespace with name determined by <code>dtxn.parent-ns-name</code>. This is the parent namespace that holds all the distributed transactions. Each distributed transaction is represented by a namespace with prefix determined by <code>dtxn.ns-prefix</code>, followed by the transaction ID.</p> <p>For example, if there are 2 distributed transactions with IDs <code>123</code> and <code>456</code>,  you should see the following namespace hierarchy:</p> <pre><code>\u2514\u2500\u2500 sys\n    \u2514\u2500\u2500 dtxns\n        \u251c\u2500\u2500 dtxn_123\n        \u2514\u2500\u2500 dtxn_456\n</code></pre>"},{"location":"catalog/iceberg/#list-all-transactions","title":"List all transactions","text":"<p>Users can list all the distributed transactions currently in the lakehouse by doing a namespace listing of the parent namespace of all distributed transactions:</p> <pre><code>nsCatalog.listNamespace(Namespace.of(\"sys\", \"dtxns\"));\n</code></pre>"},{"location":"catalog/iceberg/#begin-a-transaction","title":"Begin a transaction","text":"<p>If you create a namespace with a prefix matching the <code>dtxn.ns-prefix</code>, and the namespace is within the system namespace, and also under the parent namespace <code>dtx.parent-ns-prefix</code>, then it is considered as beginning a distributed transaction.</p> <p>The namespace properties are used to provide runtime override options for the transaction. The following options are supported:</p> Option Name Description isolation-level The isolation level of this transaction ttl-millis The duration for which a transaction is valid in milliseconds <p>The act of creating such a namespace means to create a distributed transaction that is persisted in the lakehouse. For example, consider a user creating a transaction with ID <code>123</code> with isolation level as <code>SERIALIZABLE</code>:</p> <pre><code>nsCatalog.createNamespace(\n        Namespace.of(\"sys\", \"dtxns\", \"dtxn_123\"), \n        ImmutableMap.of(\"isolation-level\", \"serializable\"));\n</code></pre>"},{"location":"catalog/iceberg/#using-the-transaction","title":"Using the transaction","text":"<p>After creation, a user can access the specific isolated version of the lakehouse under the namespace. For example, consider a Trinity Lakehouse with namespace <code>ns1</code> and table <code>t1</code>, then the user should see a namespace <code>sys.dtxns.dtxn_123.ns1</code>  and a table <code>sys.dtxns.dtxn_123.ns1.t1</code> which the user can read and write to:</p> <pre><code>assertThat(catalog.listNamespaces(Namespace.of(\"sys\", \"dtxns\", \"dtxn_123\")))\n        .containsExactly(Namespace.of(\"sys\", \"dtxns\", \"dtxn_123\", \"ns1\"));\n\nassertThat(catalog.listTables(Namespace.of(\"sys\", \"dtxns\", \"dtxn_123\", \"ns1\")))\n        .containsExactly(TableIdentifier.of(\"sys\", \"dtxns\", \"dtxn_123\", \"ns1\", \"t1\"));\n</code></pre>"},{"location":"catalog/iceberg/#commit-a-transaction","title":"Commit a transaction","text":"<p>In order to commit this transaction, set the namesapce property <code>commit</code> to <code>true</code>:</p> <pre><code>nsCatalog.setProperties(\n        Namespace.of(\"sys\", \"dtxns\", \"dtxn_123\"), \n        ImmutbaleMap.of(\"commit\", \"true\"));\n</code></pre>"},{"location":"catalog/iceberg/#rollback-a-transaction","title":"Rollback a transaction","text":"<p>In order to rollback a transaction, perform a drop namespace:</p> <pre><code>nsCatalog.dropNamespace(\n        Namespace.of(\"sys\", \"dtxns\", \"dtxn_123\"));\n</code></pre>"},{"location":"catalog/overview/","title":"Overview","text":"<p>As a storage-based format, TrinityLake can not only be used directly as a full lakehouse without a catalog, but also as the implementation for a catalog to work with any open source or proprietary catalog API standards.</p> <p>This would be useful for users that would like to maintain API level compatibility, but are seeking for a scalable yet lightweight solution,  where TrinityLake would take care of the majority of the catalog logic and do not require any extra dependencies than the storage.  This leads to the following integration pattern:</p> <p></p> <p>For users that want a complete solution with open catalog standards,  TrinityLake offers open source catalog standard servers like an IRC server.  These servers are API compatible and serve as proxy server that uses the TrinityLake SDK to perform translation, authZ, execution, etc.</p> <p>For users that would like to customize a catalog server solution,  the TrinityLake SDK can also be used to flexibly fill in the different parts of logic in a catalog server, so that users can choose to only implement the parts of the server that they want to customize, and leave the rest to TrinityLake.</p>"},{"location":"format/key-encoding/","title":"Key Encoding","text":"<p>Note</p> <p>we use the literal <code>[space]</code> to represent the space character (hex value 20) in this document for clarity</p>"},{"location":"format/key-encoding/#system-internal-keys","title":"System Internal Keys","text":"<p>In general, system internal keys do not participate in the TrinityLake tree key sorting algorithm and always stay in  the designated node.</p>"},{"location":"format/key-encoding/#lakehouse-definition-key","title":"Lakehouse Definition Key","text":"<p>The Lakehouse definition file pointer is stored with key <code>lakehouse_def</code> in the root node.</p>"},{"location":"format/key-encoding/#previous-root-node-key","title":"Previous Root Node Key","text":"<p>The pointer to the previous root node is stored with key <code>previous_root</code> in the root node.</p>"},{"location":"format/key-encoding/#rollback-root-node-key","title":"Rollback Root Node Key","text":"<p>The pointer to the root node that was rolled back from, if the root node is created during a Rollback It is stored with key <code>rollback_from_root</code> in the root node.</p>"},{"location":"format/key-encoding/#creation-timestamp-key","title":"Creation Timestamp Key","text":"<p>The key <code>created_at_millis</code> writes the timestamp that a node is created.</p>"},{"location":"format/key-encoding/#number-of-keys-key","title":"Number of Keys Key","text":"<p>The key <code>n_keys</code> writes the number of keys that a node is currently having. This is used to determine the size of the node key table.</p>"},{"location":"format/key-encoding/#object-key","title":"Object Key","text":"<p>The object key is a UTF-8 string that uniquely identifies the object and also allows sorting it in a  lexicographical order that resembles the object hierarchy in a Lakehouse.</p>"},{"location":"format/key-encoding/#object-name","title":"Object Name","text":"<p>The object name has maximum size in bytes defined in Lakehouse definition file,  with one configuration for each type of object.</p> <p>The following UTF-8 characters are not permitted in an object name:</p> <ul> <li>any control characters (hex value 00 to 1F)</li> <li>the space character (hex value 20)</li> <li>the DEL character (hex value 7F)</li> </ul>"},{"location":"format/key-encoding/#encoded-object-name","title":"Encoded Object Name","text":"<p>When used in an object key, the object name is right-padded with space up to the maximum size  (excluding the initial byte). The maximum size of each object is defined in the Lakehouse definition file.</p> <p>For example, a namespace <code>default</code> under Lakehouse definition  <code>namespace_name_max_size_bytes=8</code> will have an encoded object name<code>[space]default[space]</code>.</p>"},{"location":"format/key-encoding/#encoded-object-definition-schema-id","title":"Encoded Object Definition Schema ID","text":"<p>The schema of the object definition has a numeric ID starting from 0,  and is encoded to a 4 character base64 string that uses the following encoding:</p> <ul> <li>Uppercase letters: A\u2013Z, with indices 0\u201325</li> <li>Lowercase letters: a\u2013z, with indices 26\u201351</li> <li>Digits: 0\u20139, with indices 52\u201361</li> <li>Special symbols: <code>+</code> and <code>/</code>, with indices 62\u201363</li> <li>Padding character <code>=</code>, which may only appear at the end of the string</li> </ul> <p>For example, schema ID <code>4</code> is encoded to <code>D===</code>.</p>"},{"location":"format/key-encoding/#object-key-format","title":"Object Key Format","text":"<p>The object key format combines the Encoded Object Name,  Encoded Object Definition Schema ID rules above to form a unique key  for each type of object. See the table below for the format for each type of object: (contents in <code>&lt;&gt;</code> should be substituted)</p> Object Type Schema ID Object ID Format Example Lakehouse 0 N/A, use Lakehouse Definition Key Namespace 1 <code>B===&lt;encoded namespace name&gt;</code> <code>B===default[space]</code> Table 2 <code>C===&lt;encoded namespace name&gt;&lt;encoded table name&gt;</code> <code>C===default[space]table[space][space][space]</code>"},{"location":"format/lakehouse-transaction/","title":"Lakehouse Transaction and ACID Enforcement","text":"<p>In this document, we describe how ACID properties are enforced at Lakehouse level, leveraging the Storage Transaction guarantees.</p>"},{"location":"format/lakehouse-transaction/#supported-isolation-levels","title":"Supported Isolation Levels","text":"<p>TrinityLake supports 2 isolation levels:</p> <ul> <li>SNAPSHOT ISOLATION</li> <li>SERIALIZABLE</li> </ul> <p>This means out of the box, TrinityLake users do not need to worry about potential dirty read, non-repeatable read and phantom read that could exist at lower isolation levels.</p> <p>Depending on tolerance of serialization anomaly like write skew, users can choose to set the isolation level to SNAPSHOT ISOLATION or SERIALIZABLE. Using SERIALIZABLE would ensure fully serial transaction history,  but reducing the transaction throughput of the whole Trinity lakehouse.</p> <p>Note</p> <p>It is possible to define and implement lower isolation level for TrinityLake. However, this is currently not a priority of the project. If you are interested in exploring such support, please feel free to publish a design proposal!</p>"},{"location":"format/lakehouse-transaction/#commit-conflict-resolution-strategy","title":"Commit Conflict Resolution Strategy","text":"<p>When there is a concurrent transaction commit failure, the failing transaction that started at version <code>v</code> should do the following:</p> <ol> <li>get the current latest version of the TrinityLake tree, denote this latest version as <code>w</code></li> <li>for each version between <code>v+1</code> and <code>w</code>, see if the changes can be applied while guaranteeing the isolation level of the transaction.<ol> <li>if all the changes can be applied up to version <code>w</code>, redo the storage commit process</li> <li>if not, the transaction has failed at the lakehouse level.</li> </ol> </li> </ol>"},{"location":"format/lakehouse-transaction/#example-concurrent-commits-to-2-different-tables","title":"Example: concurrent commits to 2 different tables","text":"<p>Consider 2 transactions, T1 updating table <code>t1</code> and T2 updating table <code>t2</code>, both started at lakehouse version 3. Assuming T1 commits successfully first, the following would happen for T2:</p> <ol> <li>T2 commit against version 4 fails, due to version 4 root node file already exists</li> <li>T2 checks the current latest version, which is 4</li> <li>T2 sees in root node file of version 4 that the last transaction made change to <code>t1</code></li> <li>T2 determines that its change in <code>t2</code> does not conflict with <code>t1</code></li> <li>T2 now applies its change against version 4 root node file</li> <li>T2 commits the new root node file against version 5</li> <li>T2 commit succeeds</li> </ol> <p>Note</p> <p>This conflcit resolution strategy is currently not fully correct.  For example, it is possible for update to <code>t2</code> to leverage infomration in <code>t1</code>,  causing the commit to be not serializable. We plan to introduce the concept of \"Object Change Definition\" to fix it, but in future iterations of the format. </p>"},{"location":"format/overview/","title":"Overview","text":"<p>Note</p> <p>The TrinityLake format specification requires specific knowledge about certain data structures, algorithms,  database system and file system concepts. If you find anything described difficult to understand, please follow  the corresponding links for further explanations, or make a contribution to help us improve this document.</p>"},{"location":"format/overview/#version","title":"Version","text":"<p>This document describes the TrinityLake format at version 0.0.0.  Please see Versioning about the versioning semantics of this format.</p>"},{"location":"format/overview/#introduction","title":"Introduction","text":"<p>The TrinityLake format defines a storage-only lakehouse  implemented using a modified version of the B-epsilon tree-based key-value map.</p> <ul> <li>The keys of this map are IDs of objects in a lakehouse</li> <li>The values of this map are location pointers to the Object Definitions </li> </ul> <p>We call such tree as the TrinityLake Tree,  and call a lakehouse implemented using the TrinityLake format as a Trinity Lakehouse.</p> <p>The TrinityLake format contains the following specifications:</p> <ul> <li>The TrinityLake tree is persisted in storage following Storage Layout Specification.</li> <li>The files in a TrinityLake tree are stored according to the Storage Location Specification.</li> <li>The TrinityLake tree is assessed and updated following the Transaction Specification.</li> <li>The keys in a TrinityLake tree follow the Key Encoding Specification.</li> <li>The object definitions in a TrinityLake tree follow the Object Definition File Specification.</li> </ul>"},{"location":"format/overview/#example","title":"Example","text":"<p>Here is an example logical representation of a TrinityLake tree:</p> <p></p> <p>This Trinity Lakehouse is a tree of order 3, with the following objects:</p> <ul> <li>Namespace <code>ns1</code><ul> <li>Table <code>table1</code></li> </ul> </li> <li>Namespace <code>ns2</code><ul> <li>Table <code>table1</code>: this is an Apache Iceberg table, which further points to its own metadata JSON file,   manifests Avro files and Parquet/Avro/ORC data files, following the Iceberg table format specification.</li> <li>Index <code>index1</code></li> </ul> </li> <li>Namespace <code>ns3</code><ul> <li>Materialized View <code>mv1</code></li> <li>Table <code>table1</code></li> </ul> </li> </ul> <p>There are also a set of write operations that are performed against the objects in the write buffer of the TrinityLake tree:</p> <ul> <li>Update the definition of existing table <code>table1</code> in <code>ns1</code></li> <li>Create a new materialized view <code>mv2</code> in namespace <code>ns2</code></li> </ul> <p>The root tree node is at version 4, and also points to the previous version of the root node of the Lakehouse. This is used for achieving time travel, rollback and snapshot export.</p>"},{"location":"format/storage-layout/","title":"Storage Layout","text":"<p>The TrinityLake tree in general follows the storage layout of N-way search tree map. In this document, we describe the details of the tree's layout in storage.</p>"},{"location":"format/storage-layout/#node-file-format","title":"Node File Format","text":"<p>Similar to a N-way search tree map,  each node of the TrinityLake tree is a node file in storage.  Each file fully describes tabular data using the Apache Arrow IPC format.</p>"},{"location":"format/storage-layout/#node-file-schema","title":"Node File Schema","text":"<p>The node file has the following schema:</p> ID Name Arrow Type Description Required? Default 1 key String Name of the key no 2 value String The value of the key no 3 pnode String Pointer to the path to the child node no 4 txn String Transaction ID for write buffer no"},{"location":"format/storage-layout/#node-file-content","title":"Node File Content","text":"<p>Each node file contains 3 sections from top to bottom:</p> <ul> <li>System internal rows</li> <li>Node key table</li> <li>Write buffer</li> </ul> <p>They all share the same node file schema above, but use it in different ways.</p>"},{"location":"format/storage-layout/#system-internal-rows","title":"System-Internal Rows","text":"<p>Only the <code>key</code> and <code>value</code> columns in the node file schema are meaningful to system internal rows, and they are required to be non-null.</p> <p>These rows are used for recording system internal information such as node creation time, version, etc. See system-internal keys for more details.</p> <p>There is no specific ordering expected for the system-internal rows, and there might be more system internal rows added over time. because the first row of the node key table must have <code>NULL</code> key and <code>NULL</code> value, readers of a node file are expected to treat all rows before this row as system internal rows.</p>"},{"location":"format/storage-layout/#node-key-table","title":"Node Key Table","text":"<p>To read the node key table, the reader should skip all system internal rows,  which means to skip all rows until it reaches the first row that has a <code>NULL</code> key and <code>NULL</code> value.</p> <p>Then based on the rules of the node key table, There are exactly <code>N</code> rows for the node key table section of the node file.</p>"},{"location":"format/storage-layout/#write-buffer","title":"Write Buffer","text":"<p>The B-epsilon tree-like write buffer of the TrinityLake tree starts after the system internal rows and the node key table rows.</p> <p>Each row in the write buffer represents a message to be applied to the TrinityLake tree. New messages are appended at the bottom of the write buffer. These rows have the following requirements:</p> <ol> <li><code>key</code> must not be <code>NULL</code></li> <li><code>transaction</code> must not be <code>NULL</code></li> <li><code>pnode</code> must be <code>NULL</code></li> <li>If <code>value</code> is <code>NULL</code>, it is a message to delete the key. If <code>value</code> is not <code>NULL</code>, it is a message to set the key to the specific value.</li> </ol> <p>Note that different from a standard B-epsilon tree, when flushing write buffer against the TrinityLake tree during a write or  compaction, the messages in the latest committed transaction will not be flushed, because it will be used for ensuring different level of isolation guarantee during Trinity Lakehouse commit phase. See Transaction and ACID Enforcement for more details.</p>"},{"location":"format/storage-layout/#node-file-size","title":"Node File Size","text":"<p>Each node is targeted for the same specific size, which is configurable in the Lakehouse definition. Based on those configurations, users can roughly estimate the size of the node key table as:</p> <pre><code>N * (\n  namespace_name_size_max_bytes + \n  table_name_size_max_bytes + \n  file_path_size_max_bytes +\n  4 bytes for schema ID with padding \n)\n</code></pre> <p>and this value must be less than the node file size. This remaining size is used as the write buffer for each node.</p> <p>For users that would like to fine-tune the performance characteristics of a TrinityLake tree, this formula can be used to readjust the node file size to achieve the desired epsilon value.</p>"},{"location":"format/storage-location/","title":"Storage Location","text":"<p>In the Storage Layout document, we have described how a TrinityLake tree is persisted in a storage. This document describes the specification for the location of persisted files.</p>"},{"location":"format/storage-location/#terminologies","title":"Terminologies","text":""},{"location":"format/storage-location/#directory-vs-prefix-notation","title":"Directory vs Prefix Notation","text":"<p>In general, TrinityLake is designed against object storage systems, and only have a prefix concept. There is no operation at TrinityLake interface level to create or manage directories in any storage systems that supports directory.</p> <p>However, we retain the conventional directory concept and syntax, and users can say a prefix, or a part of a prefix,  is a directory if the purpose of that prefix is to store more files, with <code>/</code> being the separator for different levels of directories.</p>"},{"location":"format/storage-location/#types-of-locations","title":"Types of Locations","text":"<p>We in general use 3 terminologies when describing the storage locations in TrinityLake:</p> <ul> <li>URI: a URI in TrinityLake follows the RFC-3986 specification,   but in addition obeys the object storage semantics, where special characters like <code>.</code>, <code>..</code>, etc. should be interpreted literally,   rather than having semantic file system meanings. To check if a URI is qualified to be used in TrinityLake, evaluate the URI in a   POSIX file system, and the resulting URI should be identical to the original URI. For example, <code>s3://my-bucket/../file</code> is evaluated   to <code>s3://my-file</code>, and is thus not qualified and must not be used in TrinityLake.</li> <li>Path: when used in TrinityLake, a path refers to a sequence of directories, optionally with a final file name.    Therefore, a path must be relative to a prefix, and not start with <code>/</code>.</li> <li>Location: when used in TrinityLake, a location can be either a path, or a fully qualified URI</li> </ul>"},{"location":"format/storage-location/#root-uri","title":"Root URI","text":"<p>A Trinity Lakehouse always starts at a storage Root URI, for example <code>s3://my-trinity-lakehouse/</code>. We call a storage system with a root URI a Trinity Lakehouse Storage, as all the internal information of this lakehouse must be using paths within this root URI.</p> <p>The root URI is always a directory, thus to avoid user confusion, we will always treat it as  ending with a <code>/</code> even when the user input does not. For example, if the user defines the root URI as <code>s3://my-bucket/my-lakehouse</code>, it is treated as <code>s3://my-bucket/my-lakehouse/</code> when used.</p> <p>The Lakehouse root URI is not stored within the TrinityLake format itself. It is expected that a user would specify the root location at runtime. This ensures the whole lakehouse is portable for use cases like cross-region replication.</p>"},{"location":"format/storage-location/#standard-file-paths","title":"Standard File Paths","text":""},{"location":"format/storage-location/#file-path-optimization","title":"File Path Optimization","text":"<p>A file path in the TrinityLake format is designed for optimized performance in storage, with a focus on object storage. Given an Original File Path, the Optimized File Path in storage can be calculated as the following:</p> <ol> <li>Calculate the MurMur3 hash of the file path in bytes.</li> <li>Get the first 20 bytes and convert it to binary string representation and use it as the prefix.     This maximizes the throughput in object storages like S3.</li> <li>For the first, second and third group of 4 characters in the prefix, further separated with <code>/</code>.     This maximizes the throughput in file systems when a directory listing at root location is necessary.</li> <li>Replace the <code>/</code> character in original file path by <code>-</code></li> <li>Concatenate the prefix before the file path produced in step 4 using the <code>-</code> character.</li> </ol> <p>For example, an original file path <code>my/path/my-table-definition.binpb</code> will be transformed to  <code>0101/0101/0101/10101100-my-path-my-table-definition.binpb</code>.</p> <p>Warning</p> <p>File name optimization is a write side feature, and should not be used by readers to reverse-engineer the original file name.</p>"},{"location":"format/storage-location/#non-root-node-file-path","title":"Non-Root Node File Path","text":"<p>Non-root node file paths are in the form of prefix <code>node-</code> plus a version 4 UUID with suffix <code>.arrow</code>. For example, if a UUID <code>6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8</code> is generated for the node file, the original file name of the node file will be <code>node-6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8.arrow</code>, and that further goes through the file name optimization to produce the final node file path.</p>"},{"location":"format/storage-location/#object-definition-file-path","title":"Object Definition File Path","text":"<p>Object definition file paths excluding the Lakehouse definition file path are in the form of prefix <code>{object-type}-{object-identifier}-</code> plus a version 4 UUID with suffix <code>.binpb</code>. For example, a table <code>t1</code> in namespace <code>ns1</code> and UUID <code>6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8</code> would give a path <code>table-t1-ns1-6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8.binpb</code>, and that further goes through the file name optimization to produce the final object definition file path.</p>"},{"location":"format/storage-location/#non-standard-file-paths","title":"Non-Standard File Paths","text":""},{"location":"format/storage-location/#root-node-file-path","title":"Root Node File Path","text":"<p>With CoW, the root node file name is important because every change to the tree would create a new root node file, and the root node file name can be used essentially as the version of the tree.</p> <p>TrinityLake defines that each root node has a numeric version number, and the root node is stored in a file name <code>_&lt;version_number_binary_reversed&gt;.arrow</code>. The file name is persisted in storage as is without optimization. For example, the 100th version of the root node file would be stored with name <code>_00100110000000000000000000000000.arrow</code>.</p>"},{"location":"format/storage-location/#root-node-latest-version-hint-file-path","title":"Root Node Latest Version Hint File Path","text":"<p>A file with name <code>_latest_hint.txt</code> is stored and marks the hint to the latest version of the TrinityLake tree root node file. The file name is persisted in storage as is without optimization The file contains a number that marks the presumably latest version of the tree root node, such as <code>100</code>.</p>"},{"location":"format/storage-location/#lakehouse-definition-file-path","title":"Lakehouse Definition File Path","text":"<p>Lakehouse definition file path are in the form of <code>_lakehouse_def_</code> plus a version 4 UUID with suffix <code>.binpb</code>. For example, a UUID <code>6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8</code> would give a path <code>_lakehouse_def_6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8.binpb</code>.</p>"},{"location":"format/storage-transaction/","title":"Storage Transaction and ACID Enforcement","text":"<p>In this document, we discuss how the TrinityLake integrates with any storage system  to deliver transactional features.</p>"},{"location":"format/storage-transaction/#storage-requirements","title":"Storage Requirements","text":"<p>A storage used in TrinityLake must have the following properties:</p>"},{"location":"format/storage-transaction/#basic-operations","title":"Basic Operations","text":"<p>A storage must support the following operations:</p> <ul> <li>Read a file to a given location</li> <li>Write a file to a given location</li> <li>Delete a file at a given location</li> <li>Check if a file exists at a given location</li> <li>List files sharing the same prefix</li> </ul>"},{"location":"format/storage-transaction/#mutual-exclusion-of-file-creation","title":"Mutual Exclusion of File Creation","text":"<p>A storage supports mutual exclusion of file creation when only one writer wins if there are multiple writers trying to write to the same new file. This is the key feature that TrinityLake relies on for enforcing ACID semantics during the commit process.</p> <p>This feature is widely available in most storage systems, for examples:</p> <ul> <li>On Linux File System through O_EXCL</li> <li>On Hadoop Distributed File System through atomic rename</li> <li>On Amazon S3 through IF-NONE-MATCH</li> <li>On Google Cloud Storage through IF-NONE-MATCH</li> <li>On Azure Data Lake Storage through IF-NONE-MATCH</li> </ul> <p>We can also treat key-value stores as a file system, where the key records the file path and value stores the file content bytes. This further enables the usage of systems like:</p> <ul> <li>On Amazon DynamoDB through conditional PutItem</li> <li>On Redis/Valkey through SET NX</li> </ul>"},{"location":"format/storage-transaction/#consistency","title":"Consistency","text":"<p>The Consistency aspect of ACID is enforced by the storage, and TrinityLake format requires a Strongly Consistent storage. This means all operations that modify the storage (e.g. write, delete) are committed reflected immediately in any  subsequent read operations (e.g. read, list).  For example, the TrinityLake format would not work as expected if you use it on eventually consistent systems like  Apache Cassandra without enough replication factor for read and write.</p>"},{"location":"format/storage-transaction/#durability","title":"Durability","text":"<p>The Durability aspect of ACID is enforced in the storage system and out of the control of the TrinityLake format. For example, if you use the TrinityLake format on Amazon S3, you get 99.999999999% (11 9's) durability guarantee.</p>"},{"location":"format/storage-transaction/#storage-commit-process","title":"Storage Commit Process","text":""},{"location":"format/storage-transaction/#copy-on-write","title":"Copy-on-Write","text":"<p>Modifying a TrinityLake tree in storage means modifying the content of the existing node files and creating new node files. This modification process is Copy-on-Write (CoW), because \"modifying the content\" entails reading the existing content of the node file, and rewriting a completely new node file that contains potentially parts of the existing content plus the updated content.</p>"},{"location":"format/storage-transaction/#file-immutability","title":"File Immutability","text":"<p>When performing CoW, the node files are required to be created at a new file location, rather than overwriting an existing file. This means all node files are immutable once written until deletion through garbage collection process.</p>"},{"location":"format/storage-transaction/#commit-atomicity","title":"Commit Atomicity","text":"<p>When committing a transaction, the writer does the following:</p> <ol> <li>Flush write buffers if necessary and write all impacted non-root node files</li> <li>Try to write to the root node file with new write buffer in the targeted root node file name<ol> <li>If this write is successful, the transaction is considered succeeded. Write the <code>_latest_hint</code> file with the new version with best effort.</li> <li>If this write is not successful, the transaction commit step has failed at the storage layer. Depending on the overall lakehouse transaction     level, it might be possible to rebase and retry the commit. Read Lakehouse Transaction for more details.     if rebased commit is not possible, the transaction is considered failed and not retryable.</li> </ol> </li> </ol>"},{"location":"format/storage-transaction/#storage-read-isolation","title":"Storage Read Isolation","text":"<p>Based on the commit process of TrinityLake, every change to a Trinity Lakehouse must produce a new version of the TrinityLake tree root. This is the core feature used for read isolation. A transaction, either for read or write or both, will always start with identifying the version of the TrinityLake tree  to look into. This version is determined by:</p> <ol> <li>Reading the version hint file if the file exists, or start from version 0.     This is because at step 3 of Commit Atomicity, the write of the <code>_latest_hint</code> file is not guaranteed to exist or be accurate.     For example, if two processes A and B commit sequentially at version 2 and 3, but A wrote the hint slower than B,     the hint file will be incorrect with value 2.</li> <li>Try to get files of increasing version number until the version <code>k</code> that receives a file not found error</li> <li>The version <code>k-1</code> will be the one to decide the root node file path</li> </ol> <p>All the initial object definition resolutions within the specific transaction must happen using that version of the TrinityLake tree. This ensures all the transactions begin with a specific version that is isolated from other concurrent processes. For how to commit transactions to a Trinity Lakehouse while ensuring a specific ANSI-compliant isolation level guarantee,  read Lakehouse Transaction for more details.</p>"},{"location":"format/storage-transaction/#time-travel","title":"Time Travel","text":"<p>Because the TriniyLake tree node is versioned, time travel against the tree root,  i.e. time travel against the entire Trinity Lakehouse, is possible.</p> <p>The engines should match the time travel ANSI-SQL semantics in the following way:</p>"},{"location":"format/storage-transaction/#for-system_time-as-of","title":"FOR SYSTEM_TIME AS OF","text":"<p>The timestamp-based time travel can be achieved by continuously tracing the previous root node key  to older root nodes, and check the creation timestamp key until the right root node for time travel is found.</p>"},{"location":"format/storage-transaction/#for-system_version-as-of","title":"FOR SYSTEM_VERSION AS OF","text":"<p>When the system version is a numeric value, it should map to the version of the tree root node. The root node of the specific version can directly be found based on the root node file name.</p> <p>When the system version is a string that does not resemble a numeric value, it should map to a possible exported snapshot.</p>"},{"location":"format/storage-transaction/#rollback-committed-version","title":"Rollback Committed Version","text":"<p>TrinityLake uses the roll forward technique for rolling back any committed version. If the current latest root node version is <code>v</code>, and a user would like to rollback to version <code>v-1</code>, Rollback is performed by committing a new root node with version <code>v+1</code> which is most identical to the root node file <code>v-1</code>, with the difference that the root node <code>v</code> should be recorded as the rollback root node key.</p>"},{"location":"format/storage-transaction/#snapshot-export","title":"Snapshot Export","text":"<p>A snapshot export for a Trinity Lakehouse means to export a specific version of the TrinityLake tree root node, and all the files that are reachable through that root node.</p> <p>Every time an export is created, the Lakehouse definition should be updated to record the name of the export and the root node file that the export is at.</p> <p>There are many types of export that can be achieved, because the export process can decide to stop replication at any level of the tree and call it an export. At one extreme, a process can replicate any reachable files starting at the root node. We call this a Full Export. On the other side, a process can simply replicate the specific version of tree root node,  and all other files reachable from the root node are not replicated. We call this a Minimal Export. We call any export that is in between a Partial Export.</p> <p>Any file that is referenced by both the exported snapshot and the source Lakehouse might be removed by the  Lakehouse version expiration process. With a full snapshot export, all files are replicated and dereferenced from the source Lakehouse. With a partial or minimal export, additional retention policy settings are required to make sure the version expiration process still keep those files available for a certain amount of time.</p>"},{"location":"format/versioning/","title":"Versioning","text":"<p>The TrinityLake format is released independently. This means the format version could differ from the format implementation version. For example, you can have TrinityLake format <code>1.2.0</code>, Java SDK <code>1.5.1</code>.</p>"},{"location":"format/versioning/#compatibility-definitions","title":"Compatibility Definitions","text":"<p>We in general expect there to be 2 actors, Readers and Writers. Typically, reader and writer versions are at the same version, but a system can also choose to implement a reader and writer that are of different versions.</p>"},{"location":"format/versioning/#backward-compatible","title":"Backward Compatible","text":"<p>A format version is backward compatible if the format that is produced by a lower version writer can be correctly read by a higher version reader.</p>"},{"location":"format/versioning/#forward-compatible","title":"Forward Compatible","text":"<p>A format version is forward compatible if the format that is produced by a higher version writer can be correctly read by a lower version reader.</p>"},{"location":"format/versioning/#versioning-semantics","title":"Versioning Semantics","text":"<p>The TrinityLake format uses traditional major, minor and patch versioning semantics, forming a version string like <code>1.2.3</code>. We expect to bump up:</p> <ul> <li>Major version when the format introduces forward incompatible changes.</li> <li>Minor version for any new feature release in the format that is still forward compatible.</li> <li>Patch version if there are bugs, typos, improvements in wording, additional explanations, etc. that are added to the format.</li> </ul>"},{"location":"format/versioning/#format-implementation-expectations","title":"Format Implementation Expectations","text":"<p>In general, a TrinityLake format implementation is expected to be always backward compatible for all past versions, until a past version is declared as deprecated.</p> <p>Because of the backward and forward compatibility requirement, minor and patch versions are for information only so that people can know what has been changing and update their implementations accordingly. This is also why only the major version is directly recorded in the Lakehouse definition.</p> <p>It is recommended that format implementations explicitly check the format major version and  fail the reader or writer accordingly for unsupported major version.</p>"},{"location":"format/definitions/lakehouse/","title":"Lakehouse","text":"<p>Lakehouse is the top level container.</p>"},{"location":"format/definitions/lakehouse/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 0</p> Field Name Protobuf Type Description Required? Default id string A unique UUID of the lakehouse Yes major_version uint32 The major version of the format Yes 0 order uint32 The order of the B-epsilon tree Yes 128 namespace_name_max_size_bytes uint32 The maximum size of a namespace name in bytes Yes 100 table_name_max_size_bytes uint32 The maximum size of a table name in bytes Yes 100 view_name_max_size_bytes uint32 The maximum size of a view name in bytes Yes 100 node_file_max_size_bytes uint64 The maximum size of a node file in bytes Yes 1048576 (1MB) properties map Free form user-defined key-value string properties Yes txn_ttl_millis uint64 The default maximum time duration that a transaction is valid after began in millisecond Yes 604800000 (7 days) txn_isolation_level IsolationLevel The default isolation level of a transaction Yes SNAPSHOT <p>Note</p> <p>An update to some of the fields would entail a potentially expensive change of the TrinityLake tree. For example, changing the maximum object size or file size would entail re-encode all the keys in the tree.</p>"},{"location":"format/definitions/namespace/","title":"Namespace","text":"<p>A namespace is a container in a Lakehouse to organize different objects.</p>"},{"location":"format/definitions/namespace/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 1</p> Field Name Protobuf Type Description Required? Default name string A user-friendly name of this namespace Yes properties map Free form user-defined key-value string properties No"},{"location":"format/definitions/namespace/#name-size","title":"Name Size","text":"<p>All namespace names must obey the maximum size configuration defined in the Lakehouse definition file.</p>"},{"location":"format/definitions/overview/","title":"Overview","text":"<p>The TrinityLake format currently supports the following objects:</p> <ul> <li>Lakehouse</li> <li>Namespace</li> <li>Table</li> <li>View</li> <li>Transaction</li> </ul>"},{"location":"format/definitions/overview/#schema","title":"Schema","text":"<p>Each type of object definition has a different schema, which is defined using protobuf. The schema should evolve in a way that is backward and froward compatible following the versioning semantics.</p> <p>Each schema has a schema ID, and is used as a part of the TrinityLake tree key encoding.</p>"},{"location":"format/definitions/overview/#traits","title":"Traits","text":"<p>Each type of object could have different traits. Currently TrinityLake objects could have the following traits:</p> Trait Name Description Objects Assignable Another object can be assigned to belong to this object Namespace Tabular An object that presents data in the form of multiple rows where each row contains the same number of columns Table, View"},{"location":"format/definitions/overview/#file-format","title":"File Format","text":"<p>The exact definition of each object is serialized into protobuf streams binary files, suffixed with <code>.binpb</code>. These files are called Object Definition Files (ODF).</p>"},{"location":"format/definitions/transaction/","title":"Transaction","text":"<p>A transaction object in a Lakehouse represents a temporary state of the Lakehouse that is not committed yet. This object is used for distributed transaction where a transaction needs to be passed from one process to another before commit.</p>"},{"location":"format/definitions/transaction/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 4</p> Field Name Protobuf Type Description Required? Default id string A unique transaction ID Yes auto-generated UUID4 isolation_level string Isolation level of the transaction Yes SNAPSHOT beginning_root_node_file_path string File path to the beginning root node file path Yes running_root_node_file_path string File path to the root node file that represents the current state of the transaction Yes began_at_millis uint64 The millisecond epoch timestamp for which the transaction began Yes expire_at_millis uint64 The millisecond epoch timestamp for which the transaction would expire if not committed. This value must be greater than the <code>begin_at_millis</code> and cannot exceed the maximum transaction valid duration setting in the Lakehouse definition. Yes"},{"location":"format/definitions/view/","title":"View","text":"<p>A view is a tabular object that represents a transformation applied to a set of tabular objects.</p>"},{"location":"format/definitions/view/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 3</p> Field Name Protobuf Type Description Required? Default name string A user-friendly name of this view Yes schema_binding boolean If <code>true</code>, the view uses the schema defined at creation time; otherwise, it is evaluated dynamically Yes false schema Schema Schema of the view, similar to Table Schema. Required if <code>schema_binding = true</code> No sql_representations repeated SQLRepresentation Different SQL representations of the view, supporting multiple SQL dialects Yes referenced_object_ids repeated Identifier List of tabular object identifiers referenced by this view No properties map Free form user-defined key-value string properties No"},{"location":"format/definitions/view/#name-size","title":"Name Size","text":"<p>All view names must obey the maximum size configuration defined in the Lakehouse definition file.</p>"},{"location":"format/definitions/table/format-iceberg/","title":"Apache Iceberg Table Format","text":"<p>Apache Iceberg is one of the supported formats of a TrinityLake table.</p>"},{"location":"format/definitions/table/format-iceberg/#data-type-mapping","title":"Data Type Mapping","text":"<p>TrinityLake maps its data type system to Iceberg in the following way:</p> TrinityLake Type Iceberg Type boolean boolean int2 int4 integer int8 long decimal(p, s) decimal(p, s) float4 float float8 double char(n) varchar(n) string date date time3 time6 time time9 timetz3 timetz6 timetz9 timestamp3 timestamp6 timestamp timestamp9 timestamptz3 timestamptz6 timestamptz timestamptz9 fixed(n) fixed(n) varbyte(n) binary struct struct map map list list"},{"location":"format/definitions/table/format-iceberg/#managed-iceberg-table","title":"Managed Iceberg Table","text":"<p>TrinityLake managed Iceberg tables should be created without any format properties in the table definition. The TrinityLake format determines what works the best for managing an Iceberg table within a Trinity Lakehouse.</p>"},{"location":"format/definitions/table/format-iceberg/#external-iceberg-table","title":"External Iceberg Table","text":"<p>To use an external Iceberg table in TrinityLake, you can configure the following format properties:</p> Property Description Required? Default metadata_location The location of the Iceberg metadata file Yes schema_on_read If the table is schema on read. If true, a schema must be provided as a part of the table definition No false"},{"location":"format/definitions/table/format-iceberg/#federated-iceberg-table","title":"Federated Iceberg Table","text":"<p>To use a federated Iceberg table in TrinityLake, you need to configure Iceberg  catalog properties inside the format properties. TrinityLake will use the catalog properties to initialize an Iceberg catalog to federate into the external system to perform read and write. The federated table's data types will be surfaced to TrinityLake in the same way as external tables.</p>"},{"location":"format/definitions/table/overview/","title":"Overview","text":"<p>A table is a tabular object that represents a collection of related data.</p>"},{"location":"format/definitions/table/overview/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 2</p> Field Name Protobuf Type Description Required? Default name string A user-friendly name of this table Yes schema Schema Schema of the table, see Table Schema Yes distribution_keys repeated uint32 The list of IDs for columns that are used as the distribution key No sort_keys repeated uint32 The list of IDs for columns that are used as sort key No primary_key repeated uint32 The list of IDs for columns that are used as primary key No unique_columns repeated uint32 The list of IDs for columns that are not used as primary key but are unique No table_type string Table type, see Table Type No MANAGED table_format string The format of the table, which decides the usage of <code>format_properties</code>. Currently <code>ICEBERG</code> is the only option. Yes format_properties map Free form format-specific key-value string properties, e.g. Apache Iceberg No properties map Free form user-defined key-value string properties No"},{"location":"format/definitions/table/overview/#name-size","title":"Name Size","text":"<p>All table names must obey the maximum size configuration defined in the Lakehouse definition file.</p>"},{"location":"format/definitions/table/table-schema/","title":"Schema","text":""},{"location":"format/definitions/table/table-schema/#primitive-types","title":"Primitive Types","text":"<p>TrinityLake tables support the following primitive data types:</p> Type Description Aliases boolean True or false bool int2 Signed 2-byte integer smallint int4 Signed 4-byte integer int, integer int8 Signed 8-byte integer bigint, long decimal(p, s) Exact numeric of selectable precision (p) and scale (s) numeric(p, s) float4 Single precision floating-point number real float8 Double precision flaoting-point number float8, float, double char(n) Fixed length character string of size n character, nchar, bpchar varchar(n) variable length character string of optional maximum size n character varying, nvarchar, text date Calendar date of year, month, day time(p) time of a day with precision p time without time zone timetz(p) time of a day with specific time zone with precision p time with time zone timestamp(p) Date and time on a wall clock with precision p timestamp without time zone timestamptz(p) Date and time with a time zone and with precision p timestamp with time zone fixed(n) Fixed length binary value of size n binary(n) Variable length binary value of optional maximum size n varbinary, binary varying, varbyte"},{"location":"format/definitions/table/table-schema/#nested-types","title":"Nested Types","text":"<p>TrinityLake tables support the following nested data types:</p> Type Description Aliases struct A tuple of typed values row map A collection of key-value pairs with a key type and a value type list A collection of values with some element type array"},{"location":"format/definitions/table/table-type/","title":"Table Type","text":"<p>There are 3 table types in TrinityLake, MANAGED, EXTERNAL and FEDERATED.</p>"},{"location":"format/definitions/table/table-type/#managed","title":"MANAGED","text":"<p>A managed table is fully compliant with the transaction semantics defined by the TrinityLake format. It can participate in multi-object and multi-statement transactions with any other managed objects in the same Trinity Lakehouse. When dropping the table, the data is also deleted.</p> <p>TrinityLake provides the overall semantics of a managed table in areas like schema, streaming and upsert behaviors, etc. and the behavior can be implemented using various table and file formats such as Apache Iceberg with Apache Parquet.</p>"},{"location":"format/definitions/table/table-type/#external","title":"EXTERNAL","text":"<p>An external table is managed by an external system that a Trinity Lakehouse has no knowledge about. It has 4 key characteristics:</p> <ol> <li>External tables are read-only, thus do not participate in write transactions.</li> <li>The external table definition is static and requires either manual refresh or some sort of pull/push-based mechanism to trigger the refresh.</li> <li>Schema on read is possible for user to define a specific read schema that does not need to comply with the underlying data source schema.</li> <li>When dropping the table, merely the table definition is dropped, the source table in the external system remains untouched.</li> </ol>"},{"location":"format/definitions/table/table-type/#federated","title":"FEDERATED","text":"<p>A federated table is managed by an external system that a Trinity Lakehouse can connect to  and perform read or write or both through the federation connection.</p> <p>Compared to external table, federated table could support more operations such as:</p> <ul> <li>Writing to the table</li> <li>Altering or dropping the source table definition</li> <li>Always reading the latest table without the need for manual or push/pull-based refresh</li> </ul> <p>The key characteristics of federated table from ACID perspective is that, the latest version of the table for read is not determined at transaction start time, but at the time that the table is initially loaded. This is similar to the Read Latest isolation mode that we see in managed table streaming, but for federated table it is even worse because we cannot make assumptions about how \"latest\" the response really is, and we also do not know the implication of writing to such a \"latest\" table.</p> <p>Because of this behavior, compared to managed table, federated table lowers the guarantee of the transactional  semantics provided by the TrinityLake format. If a user performs multi-table transaction against a managed table  with a federated table, the isolation level would be lowered to READ UNCOMMITTED level in the worst case. For example, a federated table could be rolling back while a managed table reads its data in a  JOIN operation, causing a dirty read.</p> <p>In addition, behaviors for SQL operations like <code>DROP TABLE</code> might not be strictly defined. If the federated system does not follow the standard SQL semantics for deleting data for its managed table, there is no way for TrinityLake to enforce the strict SQL semantics.</p> <p>In summary, the FEDERATED table type provides more capabilities and stronger flexibility than EXTERNAL table type, by trading off the strong SQL transactional ACID guarantees provided by the MANAGED table type  and should be used with caution.</p>"},{"location":"format/transaction/ansi-definitions/","title":"ANSI Definitions","text":"<p>In this document, we define the anomaly phenomena and isolation levels that are  available based on the ANSI-SQL standard and used by the TrinityLake format.</p>"},{"location":"format/transaction/ansi-definitions/#dirty-read","title":"Dirty Read","text":"<p>A dirty read is defined as the following:</p> <ol> <li>Transaction T1 modifies a data item.</li> <li>Another transaction T2 then reads that data item before T1 performs a COMMIT or ROLLBACK.</li> <li>If T1 then performs a ROLLBACK, T2 has read a data item that was never committed and so never really existed.</li> </ol> <p>For example:</p> Transaction T1 Transaction T2 BEGIN; SELECT age FROM users WHERE id = 1; -- returns 20 BEGIN; UPDATE users SET age = 21 WHERE id = 1; SELECT age FROM users WHERE id = 1; -- returns 21 ROLLBACK;"},{"location":"format/transaction/ansi-definitions/#non-repeatable-read","title":"Non-Repeatable Read","text":"<p>Non-repeatable read, a.k.a. fuzzy read, is defined as the following:</p> <ol> <li>Transaction T1 reads a data item.</li> <li>Another transaction T2 then modifies or deletes that data item and commits.</li> <li>If T1 then attempts to reread the data item, it receives a modified value or discovers that the data item has been deleted.</li> </ol> <p>For example:</p> Transaction T1 Transaction T2 BEGIN; SELECT age FROM users WHERE id = 1; -- returns 20 BEGIN; UPDATE users SET age = 21 WHERE id = 1; COMMIT; SELECT age FROM users WHERE id = 1; -- returns 21"},{"location":"format/transaction/ansi-definitions/#phantom-read","title":"Phantom Read","text":"<p>Phantom read is defined as the following:</p> <ol> <li>Transaction T1 reads a set of data items satisfying some search condition.</li> <li>Transaction T2 then creates, modifies or deletes data items that satisfy T1\u2019s search condition and commits.</li> <li>If T1 then repeats its read with the same search condition, it gets a set of data items different from the first read.</li> </ol> <p>For example:</p> Transaction T1 Transaction T2 BEGIN; SELECT count(*) FROM users WHERE age &lt; 20; -- returns 2 BEGIN; INSERT INTO users (1001, 'Amy', 18) COMMIT; SELECT count(*) FROM users WHERE age &lt; 20; -- returns 3"},{"location":"format/transaction/ansi-definitions/#ansi-isolation-levels","title":"ANSI Isolation Levels","text":"<p>In ANSI standard, there are 4 isolation levels are defined based on the 3 read phenomena:</p> Isolation Level / Read Phenomenon Dirty Read Non-Repeatable Read Phantom Read READ UNCOMMITTED Possible Possible Possible READ COMMITTED Possible Possible REPEATABLE READ Possible SERIALIZABLE <p>It is pretty clear that the first 3 isolation levels directly map to the read phenomena:</p> <ul> <li>READ UNCOMMITTED level could see uncommitted data and can cause dirty read</li> <li>READ COMMITTED level does not see the dirty read phenomenon</li> <li>REPEATABLE READ level does not see the non-repeatable read phenomenon</li> </ul> <p>Note</p> <p>In some commercial solutions, the REPEATABLE READ level guarantees more than the ANSI definition.  PostgreSQL guarantees both no non-repeated and phantom read.  Oracle REPEATABLE READ is actually SERIALIZABLE. In TrinityLake, we will stick with the ANSI definition.</p> <p>Naturally, the next isolation level should be called something like \u201cno-phantom read\u201d, but it is not the case. A system with SERIALIZABLE isolation emulates serial transaction execution for all committed transactions, as if transactions had been executed one after another, serially, rather than concurrently.</p> <p>SERIALIZABLE describes the ideal state of a database system, where all concurrent transactions can act as if they are committed sequentially. The implementation of SERIALIZABLE typically involves getting locks for all related rows and also query ranges, which reduces the concurrency of query.</p>"},{"location":"format/transaction/snapshot-isolation/","title":"Snapshot Isolation","text":""},{"location":"format/transaction/snapshot-isolation/#definition","title":"Definition","text":"<p>SNAPSHOT ISOLATION is an isolation level that also avoids dirty read,  non-repeatable read and phantom read like SERIALIZABLE.</p> <p>SNAPSHOT ISOLATION allows the transactions occurring concurrently to see the same snapshot or copy of the system  as it was at the beginning of the transactions, thus allowing a second transaction to make changes to the data that  was to be read by another concurrent transaction. This other transaction would not observe the changes made by the  second transaction and would continue working on the previous snapshot of the system.</p> <p>However, unlike SERIALIZABLE, concurrent transactions might not be able to act as if they are committed sequentially. When this happens, it is called a Serialization Anomaly</p>"},{"location":"format/transaction/snapshot-isolation/#write-skew","title":"Write Skew","text":"<p>At this isolation level, a serialization anomaly called Write Skew could occur. Write skew is defined as the following:</p> <ol> <li>Suppose transaction T1 reads x and y, which are consistent with constraint C.</li> <li>Then transaction T2 reads x and y, writes x, and commits.</li> <li>Then T1 writes y.</li> <li>If there were a constraint between x and y, commit serialization might be violated.</li> </ol> <p>A famous example is the black and white marble update. Consider a table of marbles:</p> ID Color 1 Black 2 Black 3 White 4 White <p>and 2 transactions can happen in the following order:</p> Transaction T1 Transaction T2 BEGIN; UPDATE marbles set color = 'White' WHERE color = 'Black'; BEGIN; UPDATE marbles set color = 'Black' WHERE color = 'White'; COMMIT; COMMIT; <p>Under SERIALIZABLE, transaction T1 will either fail to commit,  or commit all marbles to be white to present a serial commit history of T1 after T2.</p> <p>However, under SNAPSHOT ISOLATION, both transactions would succeed without conflict,  resulting in half marbles white and half black.  This result cannot be produced by any serial execution order of T1 and T2,  which means it is a serialization anomaly.  But this does not trigger any read phenomenon including phantom read,  because the search condition of <code>where color = 'Black'</code> does not overlap with <code>where color = 'White'</code>.</p>"},{"location":"format/tree/b-epsilon-tree/","title":"B-Epsilon Tree","text":"<p>As we see from the process of updating a B-tree,  The key issue with a normal B-tree is that the write amplification could be huge. An update might change a large portion of nodes in the tree, which is not desirable for a system with a lot of writes.</p>"},{"location":"format/tree/b-epsilon-tree/#background","title":"Background","text":"<p>A B-epsilon tree is a variant of B-tree that is more optimized for writes. It was originally proposed in Lower Bounds for External Memory Dictionaries  in 2003 as a way to demonstrate an asymptotic performance tradeoff curve between B-trees and buffered repository trees. The concept is re-introduced for database applications in An Introduction to Bepsilon-trees and Write-Optimization in 2015. One application of the B-epsilon tree in the storage domain is BtrFS,  an implementation of the Linux file system using this data structure.</p>"},{"location":"format/tree/b-epsilon-tree/#write","title":"Write","text":"<p>On top of the B-tree structure, B-epsilon tree introduces a Write Buffer in each node of the tree. The write buffer holds Messages about the operations to be performed in the tree. When a write happens, instead of updating a huge portion of the tree nodes,  the writer simply writes a message in the message buffer.</p> <p>If the message buffer is full, it sends the message down the node until a node where the buffer is not full, and also applies the existing messages in the buffer to the nodes along the way if possible. This behavior is called Flushing the Write Buffer.</p>"},{"location":"format/tree/b-epsilon-tree/#read","title":"Read","text":"<p>When reading, the reader starts from the root node and go down just like in B-tree. However, in addition to walking the tree, it needs to apply any messages in the write buffers at runtime to derive the latest value of a given key. This is technically a Merge-on-Read for people that are familiar with that terminology. </p>"},{"location":"format/tree/b-epsilon-tree/#compaction","title":"Compaction","text":"<p>Because of the delayed write mechanism using write buffer, a compaction is possible against the tree, where the process can force flushing all the messages in the buffers to the corresponding keys to clear up the buffer space. The process is not necessary because eventually the writes would bring down all the write buffers  to the right nodes to be applied, but doing compaction wisely would improve the write performance further.</p>"},{"location":"format/tree/b-tree/","title":"B-Tree","text":"<p>An N-way search tree only enforces the general requirements for the number of  children per tree node. The tree could become imbalanced over time. A B-tree of order N is a self-balancing  N-way search tree that enforces a set of rules when updating the tree:</p> <ol> <li>All leaf nodes must appear at the same level</li> <li>The root node must have at least 2 children, unless it is also a leaf</li> <li>All nodes, except for the root node and leaves, must have at least <code>\u2308N/2\u2309</code> children</li> </ol>"},{"location":"format/tree/b-tree/#example","title":"Example","text":"<p>There are many tutorials online talking about B-Tree algorithms. We will not describe all details here, but just demonstrate an example of how a B-tree is built from the bottom up. Consider this B-tree of order 3 as the initial state:</p> <p></p> <p>Consider putting a new key <code>80</code> to the tree.</p> <p>We start with going down the tree and putting the value to the correct leaf.</p> <p></p> <p>Because the leaf does not satisfy the N-way search tree requirement,  we split the node and move the middle value to the parent node.</p> <p></p> <p>Now the parent node does not satisfy the N-way search tree requirement, so we split the node and move the middle value to its parent node, which makes it a root node.</p> <p></p> <p>Now the tree satisfies the B-tree definition again, so the operation is completed.</p>"},{"location":"format/tree/search-tree-map/","title":"Search Tree Map","text":"<p>A search tree can not only be used as the implementation of a set, but also a key-value Map. This can be easily achieved by storing the value together with its corresponding key.  Here is a visual example 4-way search tree map:</p> <p></p>"},{"location":"format/tree/search-tree-map/#node-key-table","title":"Node Key Table","text":"<p>For each node of the search tree map, there is an internal mapping of key to value and child node pointers. A Node Key Table provides a way to describe such information in a tabular fashion.</p> <p>Every key, value and node pointer tuple forms a row in this node key table. There are exactly <code>N</code> rows for each node in a <code>N</code>-way search tree. The construction of the table follows the rules below:</p> <ol> <li>The first row must have <code>NULL</code> key and <code>NULL</code> value, and the node pointer (if exists) points to the leftest child node.</li> <li>Subsequent rows must be filled with non-null key and value from left to right.      The node pointer at the row (if exists) points to the right child node of the key at the same row.</li> <li>If there are less than <code>N-1</code> keys available to form <code>N</code> rows, the remaining rows are filled with all <code>NULL</code> values for key,      value and node pointer.</li> </ol> <p>For example, the node map of node 1 in the example above would look like:</p> Key Value Node Pointer NULL NULL /address/to/node2 k1 v1 /address/to/node3 k2 v2 /address/to/node4 NULL NULL NULL"},{"location":"format/tree/search-tree-map/#search-tree-map-with-value-as-pointers","title":"Search Tree Map with Value as Pointers","text":"<p>For TrinityLake, it is common for the values in map to be pointers / addresses / locations  that points to a much larger payload in memory or on disk.</p> <p>Consider a key-value map, where the value for a key is a file, then a 4-way search tree map could look like the following:</p> <p></p> <p>The same node 1 would have the following node key table:</p> Key Value Node Pointer NULL NULL /address/to/node2 k1 /address/to/f1 /address/to/node3 k2 /address/to/f2 /address/to/node4 NULL NULL NULL"},{"location":"format/tree/search-tree-map/#node-file","title":"Node File","text":"<p>To persist the whole tree in storage, each node key table is stored as a separated file that we call a Node File. For example, using this mechanism, the 4-way search tree above could look like the following 4 node files in S3:</p> <p></p>"},{"location":"format/tree/search-tree/","title":"Search Tree","text":"<p>A search tree is a tree data structure used for locating specific Keys from within a collection of keys, and used as an implementation of a Set.</p> <p>A search tree consists of a collection of Nodes, and each node contains an ordered collection of keys. For example, consider a search tree with integer keys:</p> <p></p> <p>To search if the key <code>64</code> is contained within the set:</p> <ol> <li>Start from the top node <code>(1, 31)</code>, <code>64</code> is greater than <code>31</code> so go right</li> <li>In <code>(58, 101)</code>, <code>64</code> is greater than <code>58</code> but smaller than <code>101</code>, so go down the pointer between <code>58</code> and <code>101</code></li> <li>In <code>(60, 64, 77)</code>, we find exact value for <code>64</code></li> <li>Conclusion reached that key <code>64</code> is contained within the set</li> </ol>"},{"location":"format/tree/search-tree/#n-way-search-tree","title":"N-Way Search Tree","text":"<p>A N-way search tree is a search tree where:</p> <ol> <li>A node with <code>k</code> children must have <code>k-1</code> number of keys.</li> <li>Each node must have a maximum of <code>N</code> child nodes (i.e. each node must have a maximum of <code>N-1</code> keys)</li> </ol> <p>The example above thus satisfies the requirement of being a 4-way search tree.</p>"},{"location":"spark/iceberg/","title":"Iceberg Connector","text":"<p>TrinityLake can be used through the Spark Iceberg connector by leveraging the  TrinityLake Iceberg Catalog or  TrinityLake Iceberg REST Catalog integrations.</p>"},{"location":"spark/iceberg/#configuration","title":"Configuration","text":""},{"location":"spark/iceberg/#trinitylake-iceberg-catalog","title":"TrinityLake Iceberg Catalog","text":"<p>To configure an Iceberg catalog with TrinityLake in Spark, you should:</p> <ul> <li>Add TrinityLake Spark Iceberg runtime package to the Spark classpath</li> <li>Use the Spark Iceberg connector configuration for custom catalog.</li> </ul> <p>For example, to start a Spark shell session with a TrinityLake Iceberg catalog named <code>demo</code>:</p> <pre><code>spark-shell \\\n  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.0,io.trinitylake:trinitylake-spark-iceberg-runtime-3.5_2.12:0.0.1 \\\n  --conf spark.sql.extensions=org.apache.spark.iceberg.extensions.IcebergSparkSessionExtensions \\\n  --conf spark.sql.catalog.demo=org.apache.spark.iceberg.SparkCatalog \\\n  --conf spark.sql.catalog.demo.catalog-impl=io.trinitylake.iceberg.TrinityLakeIcebergCatalog \\\n  --conf spark.sql.catalog.demo.warehouse=s3://my-bucket\n</code></pre>"},{"location":"spark/iceberg/#trinitylake-iceberg-rest-catalog","title":"TrinityLake Iceberg REST Catalog","text":"<p>To configure an Iceberg REST catalog with TrinityLake in Spark, you should:</p> <ul> <li>Start your TrinityLake IRC server</li> <li>Add TrinityLake Spark Iceberg runtime package to the Spark classpath</li> <li>Add TrinityLake Spark extension to the list of Spark SQL extensions <code>io.trinitylake.spark.iceberg.TrinityLakeIcebergSparkExtensions</code></li> <li>Use the Spark Iceberg connector configuration for IRC.</li> </ul> <p>For example, to start a Spark shell session with a TrinityLake IRC catalog named <code>demo</code> that is running at <code>http://localhost:8000</code>:</p> <pre><code>spark-shell \\\n  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.0,io.trinitylake:trinitylake-spark-iceberg-runtime-3.5_2.12:0.0.1 \\\n  --conf spark.sql.extensions=org.apache.spark.iceberg.extensions.IcebergSparkSessionExtensions \\\n  --conf spark.sql.catalog.demo=org.apache.spark.iceberg.SparkCatalog \\\n  --conf spark.sql.catalog.demo.type=rest \\\n  --conf spark.sql.catalog.demo.uri=http://localhost:8000\n</code></pre>"},{"location":"spark/iceberg/#operation-behavior","title":"Operation Behavior","text":"<p>The TrinityLake Spark Iceberg connector offers the same operation behavior as the Iceberg catalog integration. See Operation Behavior in Iceberg Catalog for more details.</p>"},{"location":"spark/iceberg/#using-system-namespace","title":"Using System Namespace","text":"<p>The TrinityLake Spark Iceberg connector offers the same system namespace support as the Iceberg catalog integration to perform operations like create lakehouse and list distributed transactions. See Using System Namespace in Iceberg Catalog for more details.</p> <p>For examples:</p> <pre><code>-- create lakehouse\nCREATE DATABASE sys;\n\nSHOW DATABASES IN sys\n---------\n|name   |\n---------    \n|dtxns  |\n\n-- list distributed transactions in lakehouse\nSHOW DATABASES IN sys.dtxns\n------------\n|name      |\n------------    \n|dtxn_123  |\n|dtxn_455  |\n</code></pre>"},{"location":"spark/iceberg/#using-distributed-transaction","title":"Using Distributed Transaction","text":"<p>The Spark Iceberg connector for TrinityLake offers the same distributed transaction support as the Iceberg catalog integration using multi-level namespace. See Using Distributed Transaction in Iceberg Catalog for more details.</p> <p>For examples:</p> <pre><code>-- create a transaction with ID 1234\nCREATE DATABASE system.dtxns.dtxn_1234\n       WITH DBPROPERTIES ('isolation-level'='serializable')\n\n-- list tables in transaction of ID 1234 under namespace ns1\nSHOW TABLES IN sys.dtxns.dtxn_1234.ns1;\n------\n|name|\n------     \n|t1  |\n\nSELECT * FROM sys.dtxns.dtxn_1234.ns1.t1;\n-----------\n|id |data |\n-----------\n|1  |abc  |\n|2  |def  |\n\nINSERT INTO sys.dtxns.dtxn_1234.ns1.t1 VALUES (3, 'ghi');\n\n-- commit transaction with ID 1234\nALTER DATABASE sys.dtxns.dtxn_1234\n      SET DBPROPERTIES ('commit' = 'true');\n</code></pre>"},{"location":"spark/native/","title":"Native Connector","text":"<p>TrinityLake provides a native Spark connector using a Spark DataSource V2 (DSv2) catalog implementation for accessing a Trinity lakehouse.  It also provides Spark SQL extensions for TrinityLake specific SQL operations like transactions.</p>"},{"location":"spark/native/#configuration","title":"Configuration","text":"<p>To configure a TrinityLake Spark DSv2 catalog, you should:</p> <ul> <li>Add TrinityLake Spark runtime package <code>io.trinitylake:trinitylake-spark-runtime-3.5_2.12</code> to the Spark classpath</li> <li>Start a Spark session or application using <code>io.trinitylake.spark.TrinityLakeSparkCatalog</code> Spark catalog implementation</li> <li>Add the TrinityLake Spark SQL extensions <code>io.trinitylake.spark.TrinityLakeSparkExtensions</code> to your Spark SQL extensions list</li> <li>Set the following Spark catalog configurations:</li> </ul> Config name Description Required? Default storage.root The root URI of the TrinityLake storage Yes storage.type The type of storage No Inferred from <code>storage.root</code> scheme storage.ops.&lt;key&gt; Any property configuration for a specific type of storage operation. See Storage for more details. No txn.isolation-level The default isolation level for a transaction No default setting in Lakehouse Definition txn.ttl-millis The default duration for which a transaction is valid in milliseconds No default setting in Lakehouse Definition <p>For example:</p> <pre><code>spark-shell \\\n  --packages io.trinitylake:trinitylake-spark-runtime-3.5_2.12:0.0.1 \\\n  --conf spark.sql.extensions=io.trinitylake.spark.TrinityLakeSparkExtensions \\\n  --conf spark.sql.catalog.demo=io.trinitylake.spark.TrinityLakeSparkCatalog \\\n  --conf spark.sql.catalog.demo.storage.root=s3://my-bucket\n</code></pre>"},{"location":"spark/native/#sql-extensions","title":"SQL Extensions","text":"<p>TrinityLake provides the following Spark SQL extensions for its Spark connector:</p>"},{"location":"spark/native/#create-lakehouse","title":"CREATE LAKEHOUSE","text":"<p>Create a new lakehouse at the configured storage root location.</p> <pre><code>CREATE LAKEHOUSE [ IF NOT EXISTS ]\n       [ LOCATION location ]\n       [ LHPROPERTIES ( property_key = property_value [ , ... ] ) ]\n</code></pre>"},{"location":"spark/native/#begin-transaction","title":"BEGIN TRANSACTION","text":"<p>Begin a transaction.</p> <pre><code>BEGIN [ TRANSACTION ]\n      [ IDENTIFIED BY transaction_id ]\n      [ ISOLATION LEVEL { SERIALIZABLE | SNAPSHOT } ]\n      [ TXNPROPERTIES ( property_key = property_value [ , ... ] ) ]\n</code></pre>"},{"location":"spark/native/#commit-transaction","title":"COMMIT TRANSACTION","text":"<p>Commit a transaction.  This command can only be used after executing a <code>BEGIN TRANSACTION</code> or <code>LOAD TRANSACTION</code></p> <pre><code>COMMIT [ TRANSACTION ]\n</code></pre>"},{"location":"spark/native/#save-transaction","title":"SAVE TRANSACTION","text":"<p>Save the current transaction and exit the current transaction context. This command can only be used after executing a <code>BEGIN TRANSACTION</code> or <code>LOAD TRANSACTION</code></p> <pre><code>SAVE [ TRANSACTION ]\n</code></pre>"},{"location":"spark/native/#load-transaction","title":"LOAD TRANSACTION","text":"<p>Load a transaction of a given transaction ID, and resume its transaction context.</p> <pre><code>LOAD [ TRANSACTION ]\n     IDENTIFIED BY transaction_id\n</code></pre>"},{"location":"spark/native/#set-transaction-isolation-level","title":"SET TRANSACTION ISOLATION LEVEL","text":"<p>Set the default isolation level for any new transactions in the session.</p> <pre><code>SET [ TRANSACTION ] ISOLATION LEVEL \n    { SERIALIZABLE \n    | SNAPSHOT \n    }\n</code></pre>"},{"location":"storage/local/","title":"Local","text":"<p>Local StorageOps is used to access a local disk.</p>"},{"location":"storage/local/#storageops-properties","title":"StorageOps Properties","text":"<p>Local StorageOps has currently no configurable properties.</p>"},{"location":"storage/overview/","title":"Overview","text":"<p>Storage is the most important component for the TrinityLake format. In general, the TrinityLake format is designed to be heavily cached and work together with both the local and remote storage.</p> <p>Each node file in a TrinityLake tree is around 1MB in size by default, allowing each node to be efficiently cached to local disk. With the use of Arrow IPC format, access to those locally cached files leverages Arrow's memory mapping that allows for zero-copy reads, where the data is accessed directly from the file on disk without being copied into memory,  eliminating the need for any serialization or deserialization.</p>"},{"location":"storage/overview/#lakehouse-storage-vs-storage-operations","title":"Lakehouse Storage vs Storage Operations","text":"<p>When using TrinityLake SDKs, it is important to understand the distinction of LakehouseStorage vs StorageOperations (a.k.a. StorageOps).</p> <p>A LakehouseStorage describes operations that can be performed against paths after a Lakehouse root. Due to the nature of TrinityLake's design, it is expected that a LakehouseStorage is able to  handle the cache-based interaction across remote and local storages. </p> <p>A StorageOps describes the operations that can be performed against a specific storage solution like local disk or Amazon S3. It is implemented without any context of a lakehouse or root location.</p>"},{"location":"storage/overview/#storageops-common-properties","title":"StorageOps Common Properties","text":"<p>Each StorageOps has its own set of configurable properties. The following properties are common and should be respected by all types of storage ops:</p> Property Key Description Default delete.batch-size Default batch size for each call for batch file deletion 100 prepare-read.cache-size The cache size for pulling remote storage files to local to prepare for a read 1000 prepare-read.cache-expiration-millis The duration that a cached file is safe from cache eviction in milliseconds 600000 (10 minutes) prepare-read.staging-dir Staging directory used for read caching JVM <code>java.io.tmpdir</code> configuration for Java SDK write.staging-dir Staging directory used for write JVM <code>java.io.tmpdir</code> configuration for Java SDK"},{"location":"storage/s3/","title":"Amazon S3","text":"<p>Amazon S3 StorageOps is used to access Amazon S3, or any Amazon S3 wire-compatible storage systems like Apache Ozone. This StorageOps is implemented using the Amazon S3 async client with CRT integration for the best performance.</p>"},{"location":"storage/s3/#storageops-properties","title":"StorageOps Properties","text":"<p>Amazon S3 StorageOps has currently the following configurable properties:</p> Property Key Description Default s3.region Region of the S3 client s3.access-key-id Access key ID of the S3 client s3.secret-access-key Secret access key of the S3 client s3.session-token Session token of the S3 client"}]}